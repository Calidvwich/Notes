# 1. 绪论
## 1. 机器学习概述
定义：通过分析和解释数据的模式和结构，得到某种模型，从而实现不需要人工交互的学习，推理，决策等行为。类似计算机分析数据后，仅根据输入数据给出数据驱动型建议和决策。
适用问题：范围数据挖掘，模式识别，计算机视觉，语音识别，统计分析，自然语言处理
能解决的问题：基于给定数据的预测
包括：数据清洗，特征选择，确定算法模型，参数优化，结果预测
## 2. 机器学习的分类
![[Pasted image 20250204152741.png]]
其中，一般重点关注有监督模型和无监督模型
监督学习：
	1. 分类 classification
	2. 回归 regression，prediction
无监督学习：
	1. 聚类 clustering
	2. 降维 dimensionality reduction
## 3. 机器学习方法
考虑的方面：使用什么模型，损失函数，优化算法，评估指标
模型类别：
	1. 概率模型：决策树，朴素贝叶斯等
	2. 非概率模型：感知机，支持向量机，K-means，神经网络
如果标准为判别函数是否线性，还可以重新划分为线性模型和非线性模型
	1. 线性模型：感知机，线性向量支持机，K-means
	2. 非线性模型：核支持向量机，神经网络
损失函数：计算预测结果和真实结果的差值的函数
常用的函数包含以下几类：
$$
	1. \quad0-1损失函数：L(y,f(x))=\begin{cases} 1, & \text {if $Y\ne f(x)$ } \\ 0, &\text{if $Y=f(x)$ } \end{cases}
$$
$$
	2. \quad平方损失函数： L(y,f(x))=(Y=f(x))^2
$$
$$
	3. \quad绝对损失函数：L(y,f(x))=\left\vert Y-f(x) \right\vert
$$
优化算法：
常用的有：梯度下降法，牛顿法，拟牛顿法等
评估指标：
定义$准确率=\frac{预测正确的样本数}{样本总数}$, 0~1之间，越大越好
## 4. 机器学习的开发流程
参考人脑的学习流程![[Pasted image 20250204155712.png]]

# 2. 逻辑回归
## 1. 逻辑回归的概念
定义：又称logistic回归分析，是一种广义的线性回归分析模型，属于监督学习
运算流程：类似回归过程，通过给定的N组训练集来训练模型，并在训练后对给定的测试集（一组或多组数据）进行分类，每组数据由n个指标构成
多用于二分类/多分类问题
## 2. 线性回归
定义：通过属性的线性组合来进行预测的线性模型。使用一条直线/平面/超平面，使得预测值和真实值之间的误差最小化
满足最小二乘法
分类：
1. 二分类：将数据分为两类，找到一条直线边界让最多的两类点分局两侧
2. 多分类：其中一类是正类，剩下所有类成为负类，重复这个二分类过程。因此，n分类需要n-1次
## 3. 逻辑回归
最常用，一般用来衡量其他模型效果
主要思想：根据现有数据对于边界建立回归公式并且进行分类，需要找到最佳拟合参数
三个目的：
	1. 预测结果等于1（坏样本）的概率
	2. 对结果或预测进行分类
	3. 评估模型预测的相关概率和风险
优点：
	1. 简单高效，计算速度快，易于实现，并行程度高，在大规模数据情况下非常适用
	2. 直接对分类进行建模，无需实现假设数据分布，避免了假设分布不正确带来的问题
	3. 以概率的形式输出，并非只是0和1，由概率意义
	4. 有很好的数学性质，很多算法可以用来求最优解，训练速度快
	5. 模型清晰，有利于理解数据
缺点：
	1. 对共线性十分敏感，自变量的自相关程度高时会导致估计的误差膨胀，需要使用VIF（方差膨胀因子）来进行检验是否存在多重共线性
	2. 容易欠拟合，大多数情况下需要手动进行特征工程，构建组合特征，分类精度可能不高
	3. 本质是一个线性分类器，处理不好特征之间相关的情况
构造函数/sigmoid函数：$f(z)=\frac{1}{1+e^{-z}}$
本质思想：对于二分类问题，如果样本输入是标量且设置样本的标签值是1/0，能否直接预测一个样本属于1的概率值
![[Pasted image 20250204173916.png]]
设置分布函数
![[Pasted image 20250204174254.png]]
分类规则![[Pasted image 20250204174325.png]]
对率函数是任意阶可导的凸函数
## 4. 梯度下降确定最优解
基本思想：如果目标是凸函数，那么到最低点的最快路径必定是梯度相反的方向
# 3. 集成学习算法
## 1. 概论
起源：强学习和弱学习等价定理 （Kearns and Validant, 1989）：可以将多个分类器模型组合到一起，得到一个具有更好泛化能力的强学习器模型
原因：决策树构建比较复杂，在数据处理的时候容易过拟合，因此出现数据干扰的时候会对决策树产生极大影响，容易达到局部最优但是不容易达到全局最优。因此要综合多个分类器的结果，从而得到一个更好的分类器
分类：
1. bagging方法：从数据集中多次反复采样，独立的训练多个分类器，再采用投票法得到结果最多的分类
2. boosting方法：不仅构建新模型，新模型还更加关注前一个模型中被错误分类的样本，最终根据分类好的结果进行加权得到一个新的结果
3. stacking方法：不常用，属于分层的学习框架。在最终形成预测前，从一组学习器向另一组学习器提供信息
## 2. 随机森林
定义：基于决策树，随机子空间和bagging类集成学习思想得到的机器学习算法，包含了多颗决策树的组合模型通过弱分类器组成的一个强分类器，从而提高精确度。
由于每颗决策树的生成随机选取，因此精度能够提高
建模步骤：
1. 随机选择样本（放回抽样）
2. 随机选择特征
3. 构建决策树，重复1~3步n次，得到n棵树
4. 随机/平均森林投票
优点：
1. 能解决分类和回归两种类型的问题，并且在二者方面都有相当好的估计表现，泛化性能优越
2. 对于高维数据集的处理很好，并确定最重要的变量，是一个不错的降维方法，并可以输出特征的重要性程度
3. 可以应对缺失数据的情况，不需要归一化
4. 存在分类不平衡的时候可以有效平衡数据集误差
5. 训练速度快，高度并行化，利于分布式实现
缺点：
1. 在解决回归问题时不能给出连续性的输出。进行回归时，随机森林无法做出超越数据集的预测，可能导致对某些有特定噪声的数据集产生过拟合
2. 随机森林无法控制内部的运行，只能在不同参数和随机种子之间尝试
3. 忽略特征之间的相关性，可能有很多相似的决策树，掩盖真实结果
## 3. GBDT/梯度提升树
定义：利用损失函数的负梯度方向在当前模型的值作为残差的近似值，进而拟合一棵CART树。通过每次训练减少上一次的残差，因此需要在减少残差的梯度方向训练一个新的模型。属于boosting算法
每轮迭代的目标是找到一个CART的弱学习器，让本轮的损失最小
![[Pasted image 20250204190526.png]]
优点：
1. 既可以处理连续值也可以处理离散值
2. 调参时间短
3. 预测准确率相对较高
4. 损失函数较多，对异常值的鲁棒性较强
缺点：
1. 梯度方向上没有做到最优步长
2. 有过拟合的风险
3. 数据维度较高的时候算法的计算复杂度较高
损失函数：需要根据样本的具体特点来进行具体选择
我们选用均方差损失函数$l(y_i,f(x_i))=\frac{1}{2}(y_i-f(x_i))^2$
## 4. XGBoost/极度提升树
定义：集成多个基学习器形成强学习器，基学习器包括CART回归树和线性分类器。是一种基于决策树的分布式高效梯度提升算法
基本思想：
1. 许多弱分类器的组合是一个强分类器；最终预测值为所有树的预测值的加权和
2. 不断在错误中学习，用迭代来降低犯错概率，采用GBDT算法中基于残差建模的思想，通过多次迭代达到较高的精确度
核心思想：通过不断迭代来增加树，每次添加的树都是在拟合上次预测的残差。当训练完成得到K棵树时，如果想要对样本进行预测，只需将每个样本的特征带入模型，样本在每棵树中会落到对应的叶子节点，同时每个叶子节点对应一个分数，将各个分数相加就得到样本预测值
优点：
1. 对于输入要求不敏感，可以有缺失值
2. 不需要特征的归一化
3. 可以自动进行特征选择
4. 并行处理，在计算速度和准确率上表现良好，相比于GBDT有明显提升且复杂度不高
缺点：
1. 算法参数过多
2. 不适合处理高维特征数据
3. 模型的解释性不好
XGBoost相比于GBDT的优化点：
1. 目标函数增加了正则项，防止过拟合
2. 目标函数的优化利用了自身损失函数关于预测函数的二阶导数，增加了可利用的信息量
3. 在前二者的基础上还对节点分裂，数据处理，并行计算，内存读取等方面都有优化
