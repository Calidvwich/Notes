# 1. 绪论
## 1. 机器学习概述
定义：通过分析和解释数据的模式和结构，得到某种模型，从而实现不需要人工交互的学习，推理，决策等行为。类似计算机分析数据后，仅根据输入数据给出数据驱动型建议和决策。
适用问题：范围数据挖掘，模式识别，计算机视觉，语音识别，统计分析，自然语言处理
能解决的问题：基于给定数据的预测
包括：数据清洗，特征选择，确定算法模型，参数优化，结果预测
## 2. 机器学习的分类
![[Pasted image 20250204152741.png]]
其中，一般重点关注有监督模型和无监督模型
监督学习：
	1. 分类 classification
	2. 回归 regression，prediction
无监督学习：
	1. 聚类 clustering
	2. 降维 dimensionality reduction
## 3. 机器学习方法
考虑的方面：使用什么模型，损失函数，优化算法，评估指标
模型类别：
	1. 概率模型：决策树，朴素贝叶斯等
	2. 非概率模型：感知机，支持向量机，K-means，神经网络
如果标准为判别函数是否线性，还可以重新划分为线性模型和非线性模型
	1. 线性模型：感知机，线性向量支持机，K-means
	2. 非线性模型：核支持向量机，神经网络
损失函数：计算预测结果和真实结果的差值的函数
常用的函数包含以下几类：
$$
	1. \quad0-1损失函数：L(y,f(x))=\begin{cases} 1, & \text {if $Y\ne f(x)$ } \\ 0, &\text{if $Y=f(x)$ } \end{cases}
$$
$$
	2. \quad平方损失函数： L(y,f(x))=(Y=f(x))^2
$$
$$
	3. \quad绝对损失函数：L(y,f(x))=\left\vert Y-f(x) \right\vert
$$
优化算法：
常用的有：梯度下降法，牛顿法，拟牛顿法等
评估指标：
定义$准确率=\frac{预测正确的样本数}{样本总数}$, 0~1之间，越大越好
## 4. 机器学习的开发流程
参考人脑的学习流程![[Pasted image 20250204155712.png]]

# 2. 逻辑回归
## 1. 逻辑回归的概念
定义：又称logistic回归分析，是一种广义的线性回归分析模型，属于监督学习
运算流程：类似回归过程，通过给定的N组训练集来训练模型，并在训练后对给定的测试集（一组或多组数据）进行分类，每组数据由n个指标构成
多用于二分类/多分类问题
## 2. 线性回归
定义：通过属性的线性组合来进行预测的线性模型。使用一条直线/平面/超平面，使得预测值和真实值之间的误差最小化
满足最小二乘法
分类：
1. 二分类：将数据分为两类，找到一条直线边界让最多的两类点分局两侧
2. 多分类：其中一类是正类，剩下所有类成为负类，重复这个二分类过程。因此，n分类需要n-1次
## 3. 逻辑回归
最常用，一般用来衡量其他模型效果
主要思想：根据现有数据对于边界建立回归公式并且进行分类，需要找到最佳拟合参数
三个目的：
	1. 预测结果等于1（坏样本）的概率
	2. 对结果或预测进行分类
	3. 评估模型预测的相关概率和风险
优点：
	1. 简单高效，计算速度快，易于实现，并行程度高，在大规模数据情况下非常适用
	2. 直接对分类进行建模，无需实现假设数据分布，避免了假设分布不正确带来的问题
	3. 以概率的形式输出，并非只是0和1，由概率意义
	4. 有很好的数学性质，很多算法可以用来求最优解，训练速度快
	5. 模型清晰，有利于理解数据
缺点：
	1. 对共线性十分敏感，自变量的自相关程度高时会导致估计的误差膨胀，需要使用VIF（方差膨胀因子）来进行检验是否存在多重共线性
	2. 容易欠拟合，大多数情况下需要手动进行特征工程，构建组合特征，分类精度可能不高
	3. 本质是一个线性分类器，处理不好特征之间相关的情况
构造函数/sigmoid函数：$f(z)=\frac{1}{1+e^{-z}}$
本质思想：对于二分类问题，如果样本输入是标量且设置样本的标签值是1/0，能否直接预测一个样本属于1的概率值
![[Pasted image 20250204173916.png]]
设置分布函数
![[Pasted image 20250204174254.png]]
分类规则![[Pasted image 20250204174325.png]]
对率函数是任意阶可导的凸函数
## 4. 梯度下降确定最优解
基本思想：如果目标是凸函数，那么到最低点的最快路径必定是梯度相反的方向
# 3. 决策树
## 1. 概论
定义：由一个决策图和可能的结果（包含资源和风险）组成，用来创造达到目标的规划
内容：随机事件结果，资源代价，实用性
作用：一般用来辅助决策，在没有完备知识的情况下应该平行于概率模型作为最佳的选择模型
1. 确定一个最可能实现目标的战略
2. 计算条件概率的描述性手段
![[Pasted image 20250205133722.png]]
## 2. 原理
决策树是一种非参数的监督学习方法，通过对训练数据的学习挖掘出一定的规则用于对新的数据集进行预测。本质上，决策树可以视为若干个if...then...条件判断的集合，从根节点到任意一个叶子节点本质上是一条相应规则和对应的条件。因此，整个集合将特征空间分割为了若干个区域，落在同一个叶子节点的样本具有相同的预测值。
一般而言决策树的目标是让分支节点包含的样本尽可能属于同一类别，算法一般通过递归选择最优特征，并利用该特征对训练数据集进行分割来实现最优的分类过程
优点：
1. 速度快
2. 准确性高
3. 可以处理连续字段和种类字段
4. 不需要任何领域知识和参数假设
5. 适合高维数据
缺点：
1. 容易过拟合
2. 忽略属性之间的相关性
3. 各类别样本数量不一致的数据特征偏向于取值较多的特征
## 3. 模型算法
常用的特征选择有信息增益，信息增益率，基尼系数等
通过特征选择的方法选择最佳特征，从根节点开始递归产生决策树，不断选取局部最优特征
### 1. 基于信息增益的ID3算法
原理：在每个分裂节点选择熵值最小的特征
熵的计算：对于样本空间中的概率分布是$P_i=\frac{\left\vert C_i\right\vert}{D}$，那么数据集D的熵是$H(D)=-\Sigma_{i=1}^{n}p_i\log_2{p_i}$
其中n是标签数，指数可以以2或e为底
为了确认使用哪个特征，引入条件熵，公式为$H(D | A) = -\sum_{i=1}^{n} \frac{|D_i|}{|D|} \sum_{k=1}^{m} \frac{|D_{ik}|}{|D_i|} \log_2 \frac{|D_{ik}|}{|D_i|}$
其中，$D_i$表示D中特征A取第i个值的样本集；$D_{ik}$表示$D_{i}$中属于第k类的样本子集；存在关系等式：
$D_i=\Sigma_{k=1}^{m}D_{ik}\quad D=\Sigma_{i=1}^{n}D_i$，由此可以得到所有特征对应的条件熵值
ID3使用启发函数作为信息增益，对于不同的特征，信息增益定义为数据集的熵减去特征的条件熵，即$g(D,A)=H(D)-H(D|A)$
算法流程：
1. 给定样本集D并提取存在的特征A
2. 根据整个样本集D以及标签计算H(D)
3. 根据特征$A_i$，求解特征对应的条件熵$H(D|A_i)$以及信息增益$g(D,A_i)$
4. 选择最大的信息增益对应的特征作为当前节点的划分标准
不断重复这个流程直到叶子节点的结果类别唯一，从而建立一棵决策树
不足：使用增大信息增益作为结点的划分规划会偏向取值较多的特征
### 2. 使用信息增益率进行优化
定义为$g_g(D,A)=\frac{g(D,A)}{H_A(D)}$，其中$H_A(D)=-\Sigma_{i=1}^{n}\frac{\left\vert D_i\right\vert}{\left\vert D\right\vert}\log_2\frac{\left\vert D_i\right\vert}{\left\vert D\right\vert}$
这样取值较多的特征对应的信息增益大，但是对应取值的熵也较大，一定程度上对取值较多的特征进行了抑制。同样的，选择具有最大信息增益率
### 3. CART分类和GINI指数
该算法既可以用于回归也可用于分类，构建树叶时使用特征的取值对样本空间进行二值划分，不仅可以处理离散数据也可以处理连续数据。与信息熵相似，信息熵描述数据的混乱程度，基尼系数描述数据的纯度，系数越小信息纯度越高。分类决策树使用基尼系数进行最优特征的选择，同时决定改特征的最优二值切分点
GINI系数定义：分类问题中，假设存在K个类，样本点属于k的概率是$p_k$，那么有：$GINI(p)=\Sigma_{k=1}^{K}p_k(1-p_k)=1-\Sigma_{k=1}^{K}p_k^2,\quad其中p_k=\frac{\left\vert C_k\right\vert}{D}$，特殊的，k=2时$GINI(p)=2p(1-p)$
如果存在k个类，那么数据集D的基尼指数是$GINI(D)=1-\Sigma_{k=1}^{K}(\frac{\left\vert C_k\right\vert}{\left\vert D\right\vert})^2$
同样的，特征A的基尼指数为$GINI(D|A)=\Sigma_{k=1}^{K}\frac{|D_k|}{|D|}GINI(D_k)$
每次选择GINI指数最小的特征及其对应的切分点进行分类。如果样本D根据特征A是否取某一可能值a被分割成$D_1和D_2$两部分，GINI指数定义为$GINI(D|A)=\frac{|D_1|}{|D|}GINI(D_1)+\frac{|D_2}{|D|}GINI(D_2)$
选择最小的GINI指数作为划分标准即可，重复上述过程直到分裂节点包含的类别唯一，可以生成CART分类树。
如果特征值是连续值：将连续特征离散化，如果有m个连续值，那么从小到大排列后，会存在m-1个切分点，分别计算每个划分点下对应的GINI系数并选择数值最小的
如果特征值是离散值：在每个节点上都划分成二叉树，计算每种分类下的GINI指数，选择GINI指数最小的作为最终的特征划分
# 4. 集成学习算法
## 1. 概论
起源：强学习和弱学习等价定理 （Kearns and Validant, 1989）：可以将多个分类器模型组合到一起，得到一个具有更好泛化能力的强学习器模型
原因：决策树构建比较复杂，在数据处理的时候容易过拟合，因此出现数据干扰的时候会对决策树产生极大影响，容易达到局部最优但是不容易达到全局最优。因此要综合多个分类器的结果，从而得到一个更好的分类器
分类：
1. bagging方法：从数据集中多次反复采样，独立的训练多个分类器，再采用投票法得到结果最多的分类
2. boosting方法：不仅构建新模型，新模型还更加关注前一个模型中被错误分类的样本，最终根据分类好的结果进行加权得到一个新的结果
3. stacking方法：不常用，属于分层的学习框架。在最终形成预测前，从一组学习器向另一组学习器提供信息
## 2. 随机森林
定义：基于决策树，随机子空间和bagging类集成学习思想得到的机器学习算法，包含了多颗决策树的组合模型通过弱分类器组成的一个强分类器，从而提高精确度。
由于每颗决策树的生成随机选取，因此精度能够提高
建模步骤：
1. 随机选择样本（放回抽样）
2. 随机选择特征
3. 构建决策树，重复1~3步n次，得到n棵树
4. 随机/平均森林投票
优点：
1. 能解决分类和回归两种类型的问题，并且在二者方面都有相当好的估计表现，泛化性能优越
2. 对于高维数据集的处理很好，并确定最重要的变量，是一个不错的降维方法，并可以输出特征的重要性程度
3. 可以应对缺失数据的情况，不需要归一化
4. 存在分类不平衡的时候可以有效平衡数据集误差
5. 训练速度快，高度并行化，利于分布式实现
缺点：
1. 在解决回归问题时不能给出连续性的输出。进行回归时，随机森林无法做出超越数据集的预测，可能导致对某些有特定噪声的数据集产生过拟合
2. 随机森林无法控制内部的运行，只能在不同参数和随机种子之间尝试
3. 忽略特征之间的相关性，可能有很多相似的决策树，掩盖真实结果
## 3. GBDT/梯度提升树
定义：利用损失函数的负梯度方向在当前模型的值作为残差的近似值，进而拟合一棵CART树。通过每次训练减少上一次的残差，因此需要在减少残差的梯度方向训练一个新的模型。属于boosting算法
每轮迭代的目标是找到一个CART的弱学习器，让本轮的损失最小
![[Pasted image 20250204190526.png]]
优点：
1. 既可以处理连续值也可以处理离散值
2. 调参时间短
3. 预测准确率相对较高
4. 损失函数较多，对异常值的鲁棒性较强
缺点：
1. 梯度方向上没有做到最优步长
2. 有过拟合的风险
3. 数据维度较高的时候算法的计算复杂度较高
损失函数：需要根据样本的具体特点来进行具体选择
我们选用均方差损失函数$l(y_i,f(x_i))=\frac{1}{2}(y_i-f(x_i))^2$
## 4. XGBoost/极度提升树
定义：集成多个基学习器形成强学习器，基学习器包括CART回归树和线性分类器。是一种基于决策树的分布式高效梯度提升算法
基本思想：
1. 许多弱分类器的组合是一个强分类器；最终预测值为所有树的预测值的加权和
2. 不断在错误中学习，用迭代来降低犯错概率，采用GBDT算法中基于残差建模的思想，通过多次迭代达到较高的精确度
核心思想：通过不断迭代来增加树，每次添加的树都是在拟合上次预测的残差。当训练完成得到K棵树时，如果想要对样本进行预测，只需将每个样本的特征带入模型，样本在每棵树中会落到对应的叶子节点，同时每个叶子节点对应一个分数，将各个分数相加就得到样本预测值
优点：
1. 对于输入要求不敏感，可以有缺失值
2. 不需要特征的归一化
3. 可以自动进行特征选择
4. 并行处理，在计算速度和准确率上表现良好，相比于GBDT有明显提升且复杂度不高
缺点：
1. 算法参数过多
2. 不适合处理高维特征数据
3. 模型的解释性不好
XGBoost相比于GBDT的优化点：
1. 目标函数增加了正则项，防止过拟合
2. 目标函数的优化利用了自身损失函数关于预测函数的二阶导数，增加了可利用的信息量
3. 在前二者的基础上还对节点分裂，数据处理，并行计算，内存读取等方面都有优化
# 5. 支持向量机
## 1. 概论
SVM/支持向量机是在分类与回归分析中分析数据的监督式学习模型和相关的学习算法。给定一组训练实例，每个实例被标记为两个类别中的的一个，SVM将建立一个将新的实例分配给两个类别中的一个的模型，使其成为非概率二元线性分类器。
思想：将实例表示为空间中的点，让单独类别的实例尽可能被尽可能宽的明显间隔分隔开。此后将新的实例映射到同一空间，并基于落在间隔那一侧来判断类别。除了进行线性分类意外，还可以使用核技巧来进行非线性分类，从而将输入隐式映射到高维空间中。
目的：找到边缘向量（集合边缘上的若干数据），用这些店找出一个平面（决策面）使得支持向量到该平面的距离最大
## 2. 优缺点
优点：
1. 计算的复杂性取决于支持向量的数目而非样本空间的维度数目，避免了唯独灾难
2. 可以处理线性不可分的情况
3. 可以实现对特征空间划分的最优超平面
4. 避开了从归纳到演绎的传统特征，实现了高效的从训练样本到预报样本的推理，简化了通常过程中的分类和回归问题
5. 抓住了关键样本，剔除了大量冗余样本，有较好的鲁棒性，增加/删除支持向量样本对于模型没有影响
缺点：
1. 对大规模训练样本难以实施，样本量很大时，该矩阵的存储和计算将耗费大量的机器内存和运算时间
2. 解决多分类问题存在困难
## 3. 分类
硬间隔：完全线性可分（分割完全正确）
软间隔：允许一定量的样本分类错误
线性可分：可以用一条直线分割
线性不可分：无法用一条直线分割
### 1. 线性可分svm：
一个线性函数能将样本分开称为线性可分割
![[Pasted image 20250207102940.png]]
![[Pasted image 20250207103011.png]]
逻辑回归尝试将所有点都远离边界，SVM尝试将支持向量退的更开
### 2. 线性不可分SVM：
采用非线性变换，转换为线性问题尝试求解
![[Pasted image 20250207103152.png]]

# 6. 聚类
## 1. 概论
定义：聚类是把对象按照一定规则分成若干类，这些类并没有事先规定，而是根据数据特征确定的。类的数目和结构不需要任何假定。在同一类里对象倾向于相似。
原则：
1. 同质性：同一类中的个体要有较高的相似性
2. 互斥性：不同类中的个体要差异很大
3. 完备性：每个个体只能恰好属于一个类
## 2.  层次聚类
定义：以一定分类规则作为类间距离进行聚类
基本思想：首先考虑在没有进行聚类之前，所有参加聚类过程的个体没有归入任何类型。有了一定的原则后，可以根据个体和个体之间的距离来进行聚类，流程如图：![[Pasted image 20250208131100.png]]
![[Pasted image 20250208131528.png]]
## 3. Kmeans聚类
定义：一种无监督的分类算法，试图找到平方误差函数最小的簇
优点：
1. 适合处理大的数据集，而且数据量越大，越能显示出其优越性
2. 对于处理大数据集合，算法高效且伸缩性较好
缺点：
1. 要事先确定簇数K
2. 对初始聚类中心敏感
3. 经常以局部最优结束
4. 同时对噪声点和孤立点敏感
原理：
选择一批凝聚点/种子，让样本按照某种规则向种子点凝聚，对凝聚点进行不断修改或迭代，直至分类合理或迭代稳定为止。类的个数可以事先指定，也可以在聚类过程中确定。选择初始凝聚点的一种简单方法是采用随机抽选/随即分割样本的方法。
# 7. 特征工程和指标
## 1. 概论
特征工程：从原始数据转换为向量/训练数据的过程。典型的特征工程包括数据清理，特征提取和特征选择等
目的：获取更好的训练数据特征，使得机器学习模型逼近这个上限
作用：使模型的性能得到提升
构成：特征构建，特征提取，特征选择
重要性：各种不同算法在输入的数据达到一定量级之后都有着相近的较高准确度
## 2. 构建
### 1. 特征构建
定义：从原始数据中人工的找出一些具有物理意义的特征，在原始数据集中的特征的形式不适合直接进行建模时，使用一个或多个原特征，构造新的特征可能会比直接使用原有特征更有效
方法：经验，属性分割和组合
数据标准化：使不同规格的数据转换到统一规格的数据。标准化为了不同数据之间具备可比性，经过标准化变换之后的特征数据分布没有发生改变
包含两种常用方法：
1. 归一化（最大最小规范化）：
通过函数$x_{new}=\frac{x-x_{min}}{x_{max}-x_{min}}$将数据映射到$[0,1]$区间
数据归一化的目的是让各个特征对目标变量的影响一致，会将特征数据进行伸缩变化。因此数据归一化会影响特征数据分布
2. Zscore标准化：
要求处理后的数据均值为0，方差为1
$$Z=\frac{x-\mu}{\delta},\mu是均值，\delta是标准差$$
分箱：一般在建立模型的时候需要对连续变量离散化。在特征离散化后，模型更加稳定，降低了模型过拟合的风险
聚合特征构造：主要通过对多个特征的分组聚合实现，这些特征通常来自同一张表或者多张表的联立。常见的分组统计量有中位数，算数平均数，众数，最小值，标准差，方差，频数等
转换特征构造：使用单一特征或者多个特征进行变换后的结果作为新的特征，包含单调转换（幂转换，log变换，绝对值等）、线性组合、多项式组合比例等
### 2. 特征提取
定义：自动构建新的特征，将原始数据转换为一组具有明显物理意义或统计意义的特征
降维：包含以下两种方法
1. PCA/主成分分析
找到数据中的主成分，并利用这些主成分来表征原始数据从而达到降维的目的
2. ICA/独立成分分析
获得相互独立的属性
### 3. 特征选择
定义：从给定的特征集合中选出相关特征子集的过程
目的：去除无关特征，可以降低学习难度，简化模型，降低计算复杂度，确保不丢失重要的特征
方法：
1. 过滤式
先对数据进行特征选择然后再训练学习器，特征选择过程和后续学习器无关。先采用特征选择对初始特征进行过滤，然后用过滤后的特征训练模型
优点：计算时间上比较高效，对于过拟合问题有较高的稳健性
缺点：倾向于选择冗余特征，没有考虑到特征之间的相关性
2. 包裹式
直接把最终要使用的学习器的性能作为特征子集的评价原则，其目的是为给定学习器选择最有利于其性能的量身定做的子集
优点：直接针对特定学习器性能进行优化，考虑到特征之间的关联性，因此比过滤式能获得一个性能更好的学习器
缺点：由于需要多次训练学习器，因而计算开销更大
3. 嵌入式
将特征选择和学习器的训练过程融为一体，两者在同一个优化过程中完成。
混淆矩阵：

|          | 真实值 | positive | negative |
| -------- | --- | -------- | -------- |
| 预测值      | \   | \        | \        |
| positive | \   | TP       | FP       |
| negative | \   | FN       | TN       |
让TP, TN尽可能大，FP,FN尽可能小
## 3. 评估指标
准确率：模型预测正确的样本数占总样本数的比例$$Accurancy=\frac{TP+TN}{TP+FP+FN+TN}$$
准确率较为常用，但是当不同类别的样本占比非常不均衡的时候，占比大的类别往往成为影响准确率的最主要因素
精确度：TP占预测为P的样本的比例$$Precision=\frac{TP}{TP+FP}$$
召回率：模型预测为正类且实际为正类占实际为正类的比例$$Recall=\frac{TP}{TP+FN}$$
F1分数：精确率和召回率的调和平均数。由于精确度的上升召回率必然下降，因此使用F1来综合衡量二者。只有二者相等的时候F1分数最高$$F1-score=2\times\frac{Precision\times Recall}{Precision+Recall}$$
